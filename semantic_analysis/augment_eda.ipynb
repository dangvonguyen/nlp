{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T13:43:08.032076Z",
     "iopub.status.busy": "2025-06-28T13:43:08.031708Z",
     "iopub.status.idle": "2025-06-28T13:43:53.675302Z",
     "shell.execute_reply": "2025-06-28T13:43:53.673835Z",
     "shell.execute_reply.started": "2025-06-28T13:43:08.032043Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1177M  100 1177M    0     0   277M      0  0:00:04  0:00:04 --:--:--  284M\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p fasttext\n",
    "!curl -L -o fasttext/cc.vi.300.vec.gz https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.vi.300.vec.gz\n",
    "!gunzip fasttext/cc.vi.300.vec.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T13:43:53.678417Z",
     "iopub.status.busy": "2025-06-28T13:43:53.677116Z",
     "iopub.status.idle": "2025-06-28T13:51:12.186738Z",
     "shell.execute_reply": "2025-06-28T13:51:12.184959Z",
     "shell.execute_reply.started": "2025-06-28T13:43:53.678354Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('./fasttext/cc.vi.300.vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T13:51:12.194122Z",
     "iopub.status.busy": "2025-06-28T13:51:12.193807Z",
     "iopub.status.idle": "2025-06-28T13:51:13.772886Z",
     "shell.execute_reply": "2025-06-28T13:51:13.771690Z",
     "shell.execute_reply.started": "2025-06-28T13:51:12.194084Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Dốt', 0.6049689054489136),\n",
       " ('ngu', 0.5371081233024597),\n",
       " ('Đếck', 0.5134807229042053),\n",
       " ('giỏi', 0.502494215965271),\n",
       " ('dốt', 0.4523274302482605),\n",
       " ('học', 0.44529086351394653),\n",
       " ('xuẫn', 0.430868536233902),\n",
       " ('ngờHỏi', 0.4276396334171295),\n",
       " ('ignorant', 0.421291321516037),\n",
       " ('đoảng', 0.419640451669693)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('dốt', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T13:53:42.021745Z",
     "iopub.status.busy": "2025-06-28T13:53:42.021326Z",
     "iopub.status.idle": "2025-06-28T13:53:43.006892Z",
     "shell.execute_reply": "2025-06-28T13:53:43.005201Z",
     "shell.execute_reply.started": "2025-06-28T13:53:42.021714Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['free_text', 'label_id'],\n",
       "        num_rows: 24048\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['free_text', 'label_id'],\n",
       "        num_rows: 2672\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['free_text', 'label_id'],\n",
       "        num_rows: 6680\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('sonlam1102/vihsd')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T13:53:43.009716Z",
     "iopub.status.busy": "2025-06-28T13:53:43.009190Z",
     "iopub.status.idle": "2025-06-28T13:53:43.817808Z",
     "shell.execute_reply": "2025-06-28T13:53:43.816864Z",
     "shell.execute_reply.started": "2025-06-28T13:53:43.009684Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of clean samples: 19886\n",
      "Number of offensive samples: 1606\n",
      "Number of hate samples: 2556\n"
     ]
    }
   ],
   "source": [
    "train = pd.DataFrame(dataset['train'])\n",
    "\n",
    "clean_data = train[train['label_id'] == 0]\n",
    "offensive_data = train[train['label_id'] == 1]\n",
    "hate_data = train[train['label_id'] == 2]\n",
    "\n",
    "print(f\"Number of clean samples: {len(clean_data)}\")\n",
    "print(f\"Number of offensive samples: {len(offensive_data)}\")\n",
    "print(f\"Number of hate samples: {len(hate_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T13:53:46.145479Z",
     "iopub.status.busy": "2025-06-28T13:53:46.145153Z",
     "iopub.status.idle": "2025-06-28T13:53:46.177644Z",
     "shell.execute_reply": "2025-06-28T13:53:46.176298Z",
     "shell.execute_reply.started": "2025-06-28T13:53:46.145458Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "\n",
    "diacritic_variants = {\n",
    "    'a': ['á', 'à', 'ả', 'ã', 'ạ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ'],\n",
    "    'e': ['é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ'],\n",
    "    'i': ['í', 'ì', 'ỉ', 'ĩ', 'ị'],\n",
    "    'o': ['ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ'],\n",
    "    'u': ['ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự'],\n",
    "    'y': ['ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ'],\n",
    "    'd': ['đ']\n",
    "}\n",
    "\n",
    "reverse_diacritic_variants = {v: k for k, variants in diacritic_variants.items() for v in variants }\n",
    "\n",
    "\n",
    "def synonym_replacement(parts: list[str], n: int = 1) -> list[str]:\n",
    "    new_parts = parts.copy()\n",
    "    words = [\n",
    "        (i, word)\n",
    "        for i, word in enumerate(new_parts)\n",
    "        if word.isalnum() and word not in stop_words\n",
    "    ]\n",
    "    random.shuffle(words)\n",
    "\n",
    "    num_replaced = 0\n",
    "    for i, word in words:\n",
    "        synonyms = _get_synonyms(word)\n",
    "        if len(synonyms) == 0:\n",
    "            continue\n",
    "\n",
    "        new_parts[i] = random.choice(synonyms)\n",
    "        num_replaced += 1\n",
    "\n",
    "        if num_replaced >= n:\n",
    "            break\n",
    "\n",
    "    return new_parts\n",
    "\n",
    "\n",
    "def _get_synonyms(word: str) -> list[str]:\n",
    "    similar_words = model.most_similar(word)\n",
    "    return [word for word, score in similar_words if score > 0.5]\n",
    "\n",
    "\n",
    "def random_insertion(parts: list[str], n: int = 1) -> list[str]:\n",
    "    new_parts = parts.copy()\n",
    "    for _ in range(n):\n",
    "        new_parts = _add_random_synonym(new_parts)\n",
    "    return get_parts(''.join(new_parts))\n",
    "\n",
    "\n",
    "def _add_random_synonym(parts: list[str]) -> list[str]:\n",
    "    words = [word for word in parts if word.isalnum()]\n",
    "\n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    while len(synonyms) < 1 and len(words) > 0:\n",
    "        random_word = random.choice(words)\n",
    "\n",
    "        try:\n",
    "            synonyms = _get_synonyms(random_word)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "\n",
    "    if len(words) > 0:\n",
    "        random_synonym = synonyms[0]\n",
    "        random_idx = random.randint(0, len(words) - 1)\n",
    "        parts.insert(random_idx, ' ')\n",
    "        parts.insert(random_idx, random_synonym)\n",
    "        parts.insert(random_idx, ' ')\n",
    "    \n",
    "    return parts\n",
    "\n",
    "\n",
    "def random_swap(parts: list[str], n: int = 1) -> list[str]:\n",
    "    new_parts = parts.copy()\n",
    "    ids = [i for i, token in enumerate(new_parts) if not token.isspace()]\n",
    "    for _ in range(n):\n",
    "        if len(ids) >= 2:\n",
    "            id_1, id_2 = random.sample(ids, 2)\n",
    "            new_parts[id_1], new_parts[id_2] = new_parts[id_2], new_parts[id_1]\n",
    "    return new_parts\n",
    "\n",
    "\n",
    "def random_deletion(parts: list[str], p: float = 0.1) -> list[str]:\n",
    "    if len(parts) <= 1:\n",
    "        return parts\n",
    "\n",
    "    new_parts = []\n",
    "    for part in parts:\n",
    "        if random.uniform(0, 1) < p and not part.isspace():\n",
    "            continue\n",
    "        new_parts.append(part)\n",
    "\n",
    "    if len(new_parts) == 0:\n",
    "        rand_int = random.randint(0, len(parts) - 1)\n",
    "        return parts[rand_int]\n",
    "\n",
    "    return new_parts\n",
    "\n",
    "\n",
    "def noise_replacement(parts: list[str], noise_level: float = 0.2) -> list[str]:\n",
    "    new_parts = parts.copy()\n",
    "\n",
    "    words = [(i, word) for i, word in enumerate(new_parts) if word.isalnum()]\n",
    "    random.shuffle(words)\n",
    "\n",
    "    num_words_to_change = int(len(words) * noise_level)\n",
    "\n",
    "    for _ in range(num_words_to_change):\n",
    "        idx, word = random.choice(words)\n",
    "        char_list = list(word)\n",
    "\n",
    "        for i, char in enumerate(char_list):\n",
    "            if char.lower() in reverse_diacritic_variants:\n",
    "                new_char = reverse_diacritic_variants[char.lower()]\n",
    "                new_char = new_char.upper() if char.isupper() else new_char\n",
    "                char_list[i] = new_char\n",
    "\n",
    "        new_parts[idx] = ''.join(char_list)\n",
    "\n",
    "    return new_parts\n",
    "\n",
    "\n",
    "pattern = re.compile(r'\\w+|\\s+|[^\\w\\s]+')\n",
    "\n",
    "def get_parts(text: str) -> list[str]:\n",
    "    return pattern.findall(text)\n",
    "\n",
    "\n",
    "def get_augment_data(\n",
    "    data: pd.DataFrame,\n",
    "    min_words: int = 4,\n",
    "    num_aug: int = 4,\n",
    "    alpha_sr: float = 0.1,\n",
    "    alpha_ri: float = 0.1,\n",
    "    alpha_rs: float = 0.1,\n",
    "    p_rd: float = 0.1,\n",
    "    p_nr: float = 0.2,\n",
    "    num_methods: int = 3,\n",
    ") -> list[str]:\n",
    "    augmented_data = pd.DataFrame(columns=['free_text', 'label_id'])\n",
    "    augmented_data = augmented_data.loc[:100]\n",
    "\n",
    "    def get_augmented_texts(text: str) -> list[str]:\n",
    "        parts = get_parts(str(text))\n",
    "\n",
    "        num_words = len([part for part in parts if part.isalnum()])\n",
    "        num_tokens = len([part for part in parts if not part.isspace()])\n",
    "        \n",
    "        if num_words < min_words:\n",
    "            return []\n",
    "\n",
    "        n_sr = max(1, int(alpha_sr * num_words))\n",
    "        n_ri = max(1, int(alpha_ri * num_tokens))\n",
    "        n_rs = max(1, int(alpha_rs * num_tokens))\n",
    "\n",
    "        methods = [\n",
    "            lambda x: synonym_replacement(x, n_sr),\n",
    "            lambda x: random_insertion(x, n_ri),\n",
    "            lambda x: random_swap(x, n_rs),\n",
    "            lambda x: random_deletion(x, p_rd),\n",
    "            lambda x: noise_replacement(x, p_nr),\n",
    "        ]\n",
    "\n",
    "        augmented_texts = []\n",
    "\n",
    "        for _ in range(num_aug):\n",
    "            new_parts = parts.copy()\n",
    "            for method in random.choices(methods, k=num_methods):\n",
    "                try:\n",
    "                    new_parts = method(new_parts)\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    pass\n",
    "\n",
    "            new_parts = [p for p in ''.join(new_parts).split(' ') if p.strip()]\n",
    "            augmented_text = ' '.join(new_parts)\n",
    "            augmented_texts.append(augmented_text)\n",
    "\n",
    "        return augmented_texts\n",
    "\n",
    "    for _, row in data.iterrows():\n",
    "        text = row['free_text']\n",
    "        label = row['label_id']\n",
    "        augmented_texts = get_augmented_texts(text)\n",
    "        for augmented_text in augmented_texts:\n",
    "            augmented_data.loc[len(augmented_data)] = [augmented_text, label]\n",
    "\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T15:01:00.732278Z",
     "iopub.status.busy": "2025-06-28T15:01:00.731488Z",
     "iopub.status.idle": "2025-06-28T15:01:00.757977Z",
     "shell.execute_reply": "2025-06-28T15:01:00.756854Z",
     "shell.execute_reply.started": "2025-06-28T15:01:00.732244Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1269"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for text in offensive_data['free_text'].values:\n",
    "    parts = get_parts(str(text))\n",
    "    words = [word for word in parts if word.isalnum()]\n",
    "    if len(words) >= 4:\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T15:01:01.254254Z",
     "iopub.status.busy": "2025-06-28T15:01:01.253848Z",
     "iopub.status.idle": "2025-06-28T15:01:01.501717Z",
     "shell.execute_reply": "2025-06-28T15:01:01.500199Z",
     "shell.execute_reply.started": "2025-06-28T15:01:01.254226Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T15:01:01.779315Z",
     "iopub.status.busy": "2025-06-28T15:01:01.778902Z",
     "iopub.status.idle": "2025-06-28T15:19:22.145097Z",
     "shell.execute_reply": "2025-06-28T15:19:22.144009Z",
     "shell.execute_reply.started": "2025-06-28T15:01:01.779282Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Key 'clmn' not present in vocabulary\"\n",
      "\"Key 'cmne' not present in vocabulary\"\n",
      "\"Key 'pónk' not present in vocabulary\"\n",
      "\"Key 'cljv' not present in vocabulary\"\n",
      "\"Key 'cđb' not present in vocabulary\"\n",
      "\"Key 'cđb' not present in vocabulary\"\n",
      "\"Key 'ưiii' not present in vocabulary\"\n",
      "\"Key 'đcđ' not present in vocabulary\"\n",
      "\"Key 'vailon' not present in vocabulary\"\n",
      "\"Key 'busss' not present in vocabulary\"\n",
      "\"Key 'Đr' not present in vocabulary\"\n",
      "\"Key 'Đr' not present in vocabulary\"\n",
      "\"Key 'Đr' not present in vocabulary\"\n",
      "\"Key 'lồnn' not present in vocabulary\"\n",
      "\"Key 'lồnn' not present in vocabulary\"\n",
      "\"Key 'vailon' not present in vocabulary\"\n",
      "\"Key 'yz700' not present in vocabulary\"\n",
      "\"Key 'yz700' not present in vocabulary\"\n",
      "\"Key 'hóa' not present in vocabulary\"\n",
      "\"Key 'Thúy' not present in vocabulary\"\n",
      "\"Key 'vkllllllllll' not present in vocabulary\"\n",
      "\"Key 'thuêtj' not present in vocabulary\"\n",
      "\"Key 'khỏe' not present in vocabulary\"\n",
      "\"Key 'llon' not present in vocabulary\"\n",
      "\"Key 'lonnn' not present in vocabulary\"\n",
      "\"Key 'Đeos' not present in vocabulary\"\n",
      "\"Key 'ditme' not present in vocabulary\"\n",
      "\"Key 'vlll' not present in vocabulary\"\n",
      "\"Key '杰张' not present in vocabulary\"\n",
      "\"Key '杰张' not present in vocabulary\"\n",
      "\"Key '杰张' not present in vocabulary\"\n",
      "\"Key 'vailin' not present in vocabulary\"\n",
      "\"Key 'cuccut' not present in vocabulary\"\n",
      "\"Key 'DIễn' not present in vocabulary\"\n",
      "\"Key 'lồl' not present in vocabulary\"\n",
      "\"Key 'lồl' not present in vocabulary\"\n",
      "\"Key 'Cailozcatkhong' not present in vocabulary\"\n",
      "\"Key 'Hoàii' not present in vocabulary\"\n",
      "\"Key 'đeso' not present in vocabulary\"\n",
      "\"Key 'hàq' not present in vocabulary\"\n",
      "\"Key 'lren' not present in vocabulary\"\n",
      "\"Key 'Thùy' not present in vocabulary\"\n",
      "\"Key 'ThanhHuy' not present in vocabulary\"\n",
      "\"Key 'khỏe' not present in vocabulary\"\n",
      "\"Key 'sangr' not present in vocabulary\"\n",
      "\"Key 'ddm' not present in vocabulary\"\n",
      "\"Key 'HậuHậu' not present in vocabulary\"\n",
      "\"Key 'cẹt' not present in vocabulary\"\n",
      "\"Key 'chetme' not present in vocabulary\"\n",
      "\"Key 'chetme' not present in vocabulary\"\n",
      "\"Key 'hóa' not present in vocabulary\"\n",
      "\"Key 'đũy' not present in vocabulary\"\n",
      "\"Key 'cdmm' not present in vocabulary\"\n",
      "\"Key 'vailin' not present in vocabulary\"\n",
      "\"Key 'vailin' not present in vocabulary\"\n",
      "\"Key '12tuôi' not present in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "aug_offensive = get_augment_data(\n",
    "    data=offensive_data,\n",
    "    min_words=4,\n",
    "    num_aug=4,\n",
    "    alpha_sr=0.3,\n",
    "    alpha_ri=0.3,\n",
    "    alpha_rs=0.3,\n",
    "    p_rd=0.3,\n",
    "    p_nr=0.3,\n",
    "    num_methods=3,\n",
    ")\n",
    "aug_offensive.to_csv('data/aug_offensive.csv', index=False, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T15:19:22.147234Z",
     "iopub.status.busy": "2025-06-28T15:19:22.146885Z",
     "iopub.status.idle": "2025-06-28T15:19:22.198336Z",
     "shell.execute_reply": "2025-06-28T15:19:22.197413Z",
     "shell.execute_reply.started": "2025-06-28T15:19:22.147208Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1722"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for text in hate_data['free_text'].values:\n",
    "    parts = get_parts(str(text))\n",
    "    words = [word for word in parts if word.isalnum()]\n",
    "    if len(words) >= 10:\n",
    "        count += 1\n",
    "\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-28T15:24:25.991841Z",
     "iopub.status.busy": "2025-06-28T15:24:25.991420Z",
     "iopub.status.idle": "2025-06-28T16:07:23.419523Z",
     "shell.execute_reply": "2025-06-28T16:07:23.418534Z",
     "shell.execute_reply.started": "2025-06-28T15:24:25.991816Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Key 'trụy' not present in vocabulary\"\n",
      "\"Key 'NĢOC' not present in vocabulary\"\n",
      "\"Key 'lozzz' not present in vocabulary\"\n",
      "\"Key 'đmcs' not present in vocabulary\"\n",
      "\"Key 'Tnao' not present in vocabulary\"\n",
      "\"Key 'covid' not present in vocabulary\"\n",
      "\"Key 'vayy' not present in vocabulary\"\n",
      "\"Key 'NÍ' not present in vocabulary\"\n",
      "\"Key 'NÍ' not present in vocabulary\"\n",
      "\"Key 'hóa' not present in vocabulary\"\n",
      "\"Key 'NguCo' not present in vocabulary\"\n",
      "\"Key 'Dlv' not present in vocabulary\"\n",
      "\"Key 'Dlv' not present in vocabulary\"\n",
      "\"Key '10000usd' not present in vocabulary\"\n",
      "\"Key 'Comcom' not present in vocabulary\"\n",
      "\"Key 'coviD' not present in vocabulary\"\n",
      "\"Key 'khóa' not present in vocabulary\"\n",
      "\"Key 'Hyakuya' not present in vocabulary\"\n",
      "\"Key 'lozzzz' not present in vocabulary\"\n",
      "\"Key 'lozzzz' not present in vocabulary\"\n",
      "\"Key 'khỏe' not present in vocabulary\"\n",
      "\"Key 'Trungg' not present in vocabulary\"\n",
      "\"Key 'Covid' not present in vocabulary\"\n",
      "\"Key 'Khug' not present in vocabulary\"\n",
      "\"Key 'dlike' not present in vocabulary\"\n",
      "\"Key 'Ngụy' not present in vocabulary\"\n",
      "\"Key 'Khaaaaaa' not present in vocabulary\"\n",
      "\"Key 'HDHải' not present in vocabulary\"\n",
      "\"Key 'xóa' not present in vocabulary\"\n",
      "\"Key 'Xóa' not present in vocabulary\"\n",
      "\"Key 'conquy' not present in vocabulary\"\n",
      "\"Key 'Conchodinhung' not present in vocabulary\"\n",
      "\"Key 'tòa' not present in vocabulary\"\n",
      "\"Key 'aBe' not present in vocabulary\"\n",
      "\"Key 'tịp' not present in vocabulary\"\n",
      "\"Key 'hóa' not present in vocabulary\"\n",
      "\"Key 'lồl' not present in vocabulary\"\n",
      "\"Key 'làmcc' not present in vocabulary\"\n",
      "\"Key 'clmmm' not present in vocabulary\"\n",
      "\"Key 'clmmm' not present in vocabulary\"\n",
      "\"Key 'clmmm' not present in vocabulary\"\n",
      "\"Key 'đỗn' not present in vocabulary\"\n",
      "\"Key 'thủy' not present in vocabulary\"\n",
      "\"Key 'hóa' not present in vocabulary\"\n",
      "\"Key 'hóa' not present in vocabulary\"\n",
      "\"Key 'hóa' not present in vocabulary\"\n",
      "\"Key 'Phuk' not present in vocabulary\"\n",
      "\"Key 'xóa' not present in vocabulary\"\n",
      "\"Key 'Gdinh' not present in vocabulary\"\n",
      "\"Key 'Gdinh' not present in vocabulary\"\n",
      "\"Key 'ngòai' not present in vocabulary\"\n",
      "\"Key 'hóa' not present in vocabulary\"\n",
      "\"Key 'dụngncar' not present in vocabulary\"\n",
      "\"Key 'ducanger' not present in vocabulary\"\n",
      "\"Key 'rảm' not present in vocabulary\"\n",
      "\"Key 'rảm' not present in vocabulary\"\n",
      "\"Key 'rảm' not present in vocabulary\"\n",
      "\"Key 'lđao' not present in vocabulary\"\n",
      "\"Key 'lồnnnn' not present in vocabulary\"\n",
      "\"Key 'lồnnnn' not present in vocabulary\"\n",
      "\"Key 'lồnnnn' not present in vocabulary\"\n",
      "\"Key 'mọe' not present in vocabulary\"\n",
      "\"Key 'chiich' not present in vocabulary\"\n",
      "\"Key 'chiich' not present in vocabulary\"\n",
      "\"Key 'chiich' not present in vocabulary\"\n",
      "\"Key 'hóa' not present in vocabulary\"\n",
      "\"Key 'Airvisual' not present in vocabulary\"\n",
      "\"Key 'tiernf' not present in vocabulary\"\n",
      "\"Key 'Dcmm' not present in vocabulary\"\n",
      "\"Key 'đeoa' not present in vocabulary\"\n",
      "\"Key 'đeoa' not present in vocabulary\"\n"
     ]
    }
   ],
   "source": [
    "aug_hate = get_augment_data(\n",
    "    data=hate_data,\n",
    "    min_words=10,\n",
    "    num_aug=3,\n",
    "    alpha_sr=0.3,\n",
    "    alpha_ri=0.3,\n",
    "    alpha_rs=0.3,\n",
    "    p_rd=0.3,\n",
    "    p_nr=0.3,\n",
    "    num_methods=3,\n",
    ")\n",
    "aug_hate.to_csv('data/aug_hate.csv', index=False, sep='|')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
